{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "langauge_generator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9dV/lHlbinysI9B6QpoHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rocky12312/Poetry-generation/blob/master/langauge_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKTGfO0Py0WU",
        "colab_type": "code",
        "outputId": "b4c8a8b8-ab1d-40b8-851b-a1fec0a29d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fR26fv6zBHr",
        "colab_type": "code",
        "outputId": "5971d19b-fbb9-49fe-f72e-b6e4c8426ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd drive/My\\ Drive/Poetry_generation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Poetry_generation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7cWKhn8zYlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s-wLm1I3Z9Z",
        "colab_type": "text"
      },
      "source": [
        "Reading the data from robert_frost.txt which contain poetry by Robert Frost including the most famous poems including \"Road Not Taken\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSfZx72u3YpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Will contain the text sequence which we gonna be treating as input\n",
        "input_text = []\n",
        "#Will contain the text sequence which gonna be treating as output\n",
        "output_text = []\n",
        "with open(os.path.join(\"robert_frost.txt\")) as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip()\n",
        "    if not line:\n",
        "      continue\n",
        "    #As we know that input will have a start of sentence token <sos> token and output will have a end of sentence <eos> token\n",
        "    input_line = \"<sos> \"+line\n",
        "    output_line = line+\" <eos>\"\n",
        "    #Appending the input in input_text and output in output_text which we will be using for the same purpose\n",
        "    input_text.append(input_line)\n",
        "    output_text.append(output_line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEQIY4rH6oiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the maximum_sequence_length\n",
        "Max_sequence_length = 80\n",
        "#Now joining both the input and output in corp\n",
        "corp = input_text+output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNlJliey7QPA",
        "colab_type": "code",
        "outputId": "f49a3f0e-4372-4bdc-984d-df7a39e2c45b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "Max_vocab_size = 3000\n",
        "tokenizer = Tokenizer(num_words = Max_vocab_size,filters = \" \")\n",
        "tokenizer.fit_on_texts(corp)\n",
        "#Now converting the input and output sentences to sequences of integer using tokenizer\n",
        "input_sequences = tokenizer.texts_to_sequences(input_text)\n",
        "output_sequences = tokenizer.texts_to_sequences(output_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "784HbIq78kJ9",
        "colab_type": "code",
        "outputId": "41c40416-1048-4deb-85ca-d1e8da91ff4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Finding the maximum sequence length\n",
        "Max_sequence_length_data = max(len(s) for s in input_sequences)\n",
        "print(\"max_sequence_length_from_data: \",Max_sequence_length_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_sequence_length_from_data:  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQyaZw7t9ZDU",
        "colab_type": "code",
        "outputId": "3e85fbbe-d190-45fe-d2ef-0bdb910d2431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Utilizing word to index from tokenizer(actually in sequence each word has been mapped to a certain index which actually belong to word in vocabulary)\n",
        "word2idx = tokenizer.word_index\n",
        "print(\"Length_of_word_index: \",len(word2idx))\n",
        "assert(\"<sos>\" in word2idx)\n",
        "assert(\"<eos>\" in word2idx)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length_of_word_index:  3056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDPI3adY-a5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sequence_length = min(Max_sequence_length,Max_sequence_length_data)\n",
        "#Now padding the input and output sequences(padding will be post)\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences,maxlen = max_sequence_length,padding = \"post\")\n",
        "padded_output_sequences = pad_sequences(output_sequences,maxlen = max_sequence_length,padding = \"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNYWB-Xi_-9y",
        "colab_type": "text"
      },
      "source": [
        "Now basically what we have is input and output padded sequences\n",
        "\n",
        "input sequences are vector of size 12 and so are output sequences and we have 1436 sample pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKWiloHFAzUa",
        "colab_type": "text"
      },
      "source": [
        "Now we be loading our pretrained glove word embedding and we will transform each and every vector containing words in padded sequences with the word vectors from glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA-kNA6fBKSh",
        "colab_type": "code",
        "outputId": "ca5bd7c9-8b38-4b1b-b2b0-733abcab113e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Here we will be using Glove word vectors\n",
        "#First creating a dictonary of word to vectors(initializing a empty dictionary)\n",
        "word2vec = {}\n",
        "with open(os.path.join(\"glove.6B.100d.txt\")) as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:],dtype = \"float32\")\n",
        "    word2vec[word] = vec\n",
        "print(\"No of word to vectors in glove word 2 vec: \",len(word2vec))\n",
        "#So here basically we created a dictionary having words as index and corresponding vectors as values"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of word to vectors in glove word 2 vec:  400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwWtEj_EBudC",
        "colab_type": "text"
      },
      "source": [
        "Now next step which we gonna do is create a embedding matrix which we will use for mapping each of the word of our sentence to a vector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNpqKoIwBvfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#These num_words will be our vocabulary size\n",
        "#Embedding_dim will be the size of our embeddings(word vectors), which will be 100 as in glove 100d each word contain a corresponding vector of 100d\n",
        "#This plus 1 because word2idx index start from 1\n",
        "num_words = min(Max_vocab_size,len(word2idx)+1)\n",
        "Embedding_dim = 100\n",
        "embedding_matrix = np.zeros((num_words,Embedding_dim))\n",
        "for word,i in word2idx.items():\n",
        "  if i<Max_vocab_size:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "#This is the way we gonna fill our embedding matrix with word vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUQZvx3kCEKW",
        "colab_type": "text"
      },
      "source": [
        "One hot target can't use sparse cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U0AuwZcB9-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_targets = np.zeros((len(padded_input_sequences),max_sequence_length,num_words))\n",
        "for i,target in enumerate(padded_output_sequences):\n",
        "  for t,word in enumerate(target):\n",
        "    if word>0:\n",
        "      one_hot_targets[i,t,word] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2UNXqXDhT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "embedding_dim = 100\n",
        "embedding_layer = Embedding(num_words,embedding_dim,weights = [embedding_matrix])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYLipRkhEJMQ",
        "colab_type": "text"
      },
      "source": [
        "Now creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wz4mP1ZD8vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input,LSTM,Dense\n",
        "#Defining the latent dim\n",
        "M = 25\n",
        "#For our model we will be having three inputs including the initail cell state and hidded state with input\n",
        "input_ = Input(shape = (max_sequence_length,))\n",
        "initial_h = Input(shape = (M,))\n",
        "initial_c = Input(shape = (M,))\n",
        "x = embedding_layer(input_)\n",
        "#Return_sequence True as we have to preict the full sequence\n",
        "lstm = LSTM(M,return_sequences = True,return_state = True)\n",
        "x,_,_ = lstm(x,initial_state = [initial_h,initial_c])\n",
        "dense = Dense(num_words,activation = \"softmax\")\n",
        "output = dense(x)\n",
        "#Inputs(input_,initail_h,initil_c) and Outputs(output)\n",
        "model = Model([input_,initial_h,initial_c],output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP_chLgOF-aW",
        "colab_type": "text"
      },
      "source": [
        "Thing to be given attention is that we have set return_state and return_sequences to be true so in our output we will be having output at each time stamp and return_state true will return both cell state and hidden state but if used GRU it will return only hidden state as thre is no cell state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiFAVtIhHZNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now compiling the model\n",
        "from keras.optimizers import Adam,SGD\n",
        "model.compile(loss = \"categorical_crossentropy\",optimizer = Adam(lr = 0.01),metrics = [\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b6qhjTgIGaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now defining the values of the initial hidden and cell state\n",
        "val = np.zeros((len(padded_input_sequences),M))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEG18-q3Ivxv",
        "colab_type": "code",
        "outputId": "ef984032-8cf4-4997-bb0d-dbdcc99134df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Fitting the model on the input and output data\n",
        "ht = model.fit([padded_input_sequences,val,val],one_hot_targets,batch_size = 128,epochs = 200,validation_split = 0.2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1148 samples, validate on 288 samples\n",
            "Epoch 1/200\n",
            "1148/1148 [==============================] - 3s 2ms/step - loss: 5.3717 - accuracy: 0.0703 - val_loss: 5.0344 - val_accuracy: 0.0833\n",
            "Epoch 2/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.5928 - accuracy: 0.0833 - val_loss: 4.7843 - val_accuracy: 0.0833\n",
            "Epoch 3/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.3441 - accuracy: 0.0833 - val_loss: 4.8836 - val_accuracy: 0.0833\n",
            "Epoch 4/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.2981 - accuracy: 0.0833 - val_loss: 4.8977 - val_accuracy: 0.0833\n",
            "Epoch 5/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.2460 - accuracy: 0.0833 - val_loss: 4.9331 - val_accuracy: 0.0833\n",
            "Epoch 6/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.2190 - accuracy: 0.0833 - val_loss: 4.9419 - val_accuracy: 0.0833\n",
            "Epoch 7/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.1877 - accuracy: 0.0833 - val_loss: 4.9489 - val_accuracy: 0.0833\n",
            "Epoch 8/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.1536 - accuracy: 0.0841 - val_loss: 4.9452 - val_accuracy: 0.0859\n",
            "Epoch 9/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.1129 - accuracy: 0.0904 - val_loss: 4.9537 - val_accuracy: 0.0859\n",
            "Epoch 10/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.0740 - accuracy: 0.0925 - val_loss: 4.9537 - val_accuracy: 0.0868\n",
            "Epoch 11/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.0210 - accuracy: 0.0924 - val_loss: 4.9197 - val_accuracy: 0.0862\n",
            "Epoch 12/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.9761 - accuracy: 0.0940 - val_loss: 4.9175 - val_accuracy: 0.0865\n",
            "Epoch 13/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.9284 - accuracy: 0.0939 - val_loss: 4.9010 - val_accuracy: 0.0865\n",
            "Epoch 14/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.8813 - accuracy: 0.0955 - val_loss: 4.8974 - val_accuracy: 0.0877\n",
            "Epoch 15/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.8335 - accuracy: 0.0981 - val_loss: 4.8906 - val_accuracy: 0.0888\n",
            "Epoch 16/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.7871 - accuracy: 0.1014 - val_loss: 4.8750 - val_accuracy: 0.0911\n",
            "Epoch 17/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.7418 - accuracy: 0.1056 - val_loss: 4.8696 - val_accuracy: 0.0917\n",
            "Epoch 18/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.7004 - accuracy: 0.1071 - val_loss: 4.8717 - val_accuracy: 0.0911\n",
            "Epoch 19/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.6616 - accuracy: 0.1152 - val_loss: 4.8826 - val_accuracy: 0.0952\n",
            "Epoch 20/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.6244 - accuracy: 0.1200 - val_loss: 4.8898 - val_accuracy: 0.0969\n",
            "Epoch 21/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.5898 - accuracy: 0.1224 - val_loss: 4.9034 - val_accuracy: 0.0938\n",
            "Epoch 22/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.5591 - accuracy: 0.1238 - val_loss: 4.9087 - val_accuracy: 0.0949\n",
            "Epoch 23/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.5271 - accuracy: 0.1252 - val_loss: 4.9208 - val_accuracy: 0.0946\n",
            "Epoch 24/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.4976 - accuracy: 0.1275 - val_loss: 4.9294 - val_accuracy: 0.0952\n",
            "Epoch 25/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.4677 - accuracy: 0.1294 - val_loss: 4.9451 - val_accuracy: 0.0955\n",
            "Epoch 26/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.4388 - accuracy: 0.1289 - val_loss: 4.9538 - val_accuracy: 0.0946\n",
            "Epoch 27/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.4123 - accuracy: 0.1290 - val_loss: 4.9664 - val_accuracy: 0.0955\n",
            "Epoch 28/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.3834 - accuracy: 0.1304 - val_loss: 4.9700 - val_accuracy: 0.0943\n",
            "Epoch 29/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.3561 - accuracy: 0.1326 - val_loss: 4.9758 - val_accuracy: 0.0940\n",
            "Epoch 30/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.3267 - accuracy: 0.1338 - val_loss: 4.9911 - val_accuracy: 0.0952\n",
            "Epoch 31/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.2993 - accuracy: 0.1366 - val_loss: 4.9983 - val_accuracy: 0.0940\n",
            "Epoch 32/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.2726 - accuracy: 0.1373 - val_loss: 5.0117 - val_accuracy: 0.0929\n",
            "Epoch 33/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.2467 - accuracy: 0.1418 - val_loss: 5.0172 - val_accuracy: 0.0946\n",
            "Epoch 34/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.2166 - accuracy: 0.1429 - val_loss: 5.0227 - val_accuracy: 0.0940\n",
            "Epoch 35/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.1891 - accuracy: 0.1461 - val_loss: 5.0275 - val_accuracy: 0.0946\n",
            "Epoch 36/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.1611 - accuracy: 0.1489 - val_loss: 5.0295 - val_accuracy: 0.0943\n",
            "Epoch 37/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.1345 - accuracy: 0.1485 - val_loss: 5.0371 - val_accuracy: 0.0940\n",
            "Epoch 38/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.1059 - accuracy: 0.1538 - val_loss: 5.0561 - val_accuracy: 0.0943\n",
            "Epoch 39/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.0790 - accuracy: 0.1566 - val_loss: 5.0667 - val_accuracy: 0.0949\n",
            "Epoch 40/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.0536 - accuracy: 0.1593 - val_loss: 5.0813 - val_accuracy: 0.0938\n",
            "Epoch 41/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.0292 - accuracy: 0.1634 - val_loss: 5.0937 - val_accuracy: 0.0932\n",
            "Epoch 42/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.0003 - accuracy: 0.1661 - val_loss: 5.0998 - val_accuracy: 0.0943\n",
            "Epoch 43/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.9726 - accuracy: 0.1687 - val_loss: 5.1111 - val_accuracy: 0.0943\n",
            "Epoch 44/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.9456 - accuracy: 0.1731 - val_loss: 5.1160 - val_accuracy: 0.0932\n",
            "Epoch 45/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.9216 - accuracy: 0.1767 - val_loss: 5.1318 - val_accuracy: 0.0943\n",
            "Epoch 46/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.8964 - accuracy: 0.1800 - val_loss: 5.1386 - val_accuracy: 0.0946\n",
            "Epoch 47/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.8702 - accuracy: 0.1818 - val_loss: 5.1503 - val_accuracy: 0.0943\n",
            "Epoch 48/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.8457 - accuracy: 0.1874 - val_loss: 5.1648 - val_accuracy: 0.0946\n",
            "Epoch 49/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.8214 - accuracy: 0.1929 - val_loss: 5.1758 - val_accuracy: 0.0946\n",
            "Epoch 50/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7989 - accuracy: 0.1951 - val_loss: 5.1914 - val_accuracy: 0.0946\n",
            "Epoch 51/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7752 - accuracy: 0.1982 - val_loss: 5.2020 - val_accuracy: 0.0940\n",
            "Epoch 52/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7538 - accuracy: 0.1997 - val_loss: 5.2131 - val_accuracy: 0.0926\n",
            "Epoch 53/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7304 - accuracy: 0.2025 - val_loss: 5.2220 - val_accuracy: 0.0932\n",
            "Epoch 54/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7098 - accuracy: 0.2049 - val_loss: 5.2334 - val_accuracy: 0.0929\n",
            "Epoch 55/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6889 - accuracy: 0.2087 - val_loss: 5.2438 - val_accuracy: 0.0932\n",
            "Epoch 56/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6687 - accuracy: 0.2101 - val_loss: 5.2532 - val_accuracy: 0.0926\n",
            "Epoch 57/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6500 - accuracy: 0.2133 - val_loss: 5.2720 - val_accuracy: 0.0911\n",
            "Epoch 58/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6285 - accuracy: 0.2157 - val_loss: 5.2798 - val_accuracy: 0.0914\n",
            "Epoch 59/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6095 - accuracy: 0.2204 - val_loss: 5.2875 - val_accuracy: 0.0926\n",
            "Epoch 60/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5918 - accuracy: 0.2217 - val_loss: 5.2978 - val_accuracy: 0.0906\n",
            "Epoch 61/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5733 - accuracy: 0.2228 - val_loss: 5.3147 - val_accuracy: 0.0897\n",
            "Epoch 62/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5539 - accuracy: 0.2274 - val_loss: 5.3262 - val_accuracy: 0.0914\n",
            "Epoch 63/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5356 - accuracy: 0.2289 - val_loss: 5.3351 - val_accuracy: 0.0911\n",
            "Epoch 64/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5165 - accuracy: 0.2315 - val_loss: 5.3494 - val_accuracy: 0.0883\n",
            "Epoch 65/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4989 - accuracy: 0.2324 - val_loss: 5.3543 - val_accuracy: 0.0903\n",
            "Epoch 66/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4820 - accuracy: 0.2355 - val_loss: 5.3742 - val_accuracy: 0.0888\n",
            "Epoch 67/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4661 - accuracy: 0.2390 - val_loss: 5.3801 - val_accuracy: 0.0880\n",
            "Epoch 68/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4485 - accuracy: 0.2409 - val_loss: 5.3981 - val_accuracy: 0.0880\n",
            "Epoch 69/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4310 - accuracy: 0.2426 - val_loss: 5.4038 - val_accuracy: 0.0894\n",
            "Epoch 70/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4159 - accuracy: 0.2448 - val_loss: 5.4212 - val_accuracy: 0.0868\n",
            "Epoch 71/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3999 - accuracy: 0.2483 - val_loss: 5.4245 - val_accuracy: 0.0880\n",
            "Epoch 72/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3847 - accuracy: 0.2485 - val_loss: 5.4425 - val_accuracy: 0.0877\n",
            "Epoch 73/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3718 - accuracy: 0.2512 - val_loss: 5.4407 - val_accuracy: 0.0888\n",
            "Epoch 74/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3572 - accuracy: 0.2532 - val_loss: 5.4519 - val_accuracy: 0.0851\n",
            "Epoch 75/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3396 - accuracy: 0.2572 - val_loss: 5.4581 - val_accuracy: 0.0848\n",
            "Epoch 76/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3236 - accuracy: 0.2578 - val_loss: 5.4793 - val_accuracy: 0.0862\n",
            "Epoch 77/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3085 - accuracy: 0.2599 - val_loss: 5.4879 - val_accuracy: 0.0871\n",
            "Epoch 78/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2945 - accuracy: 0.2618 - val_loss: 5.5019 - val_accuracy: 0.0851\n",
            "Epoch 79/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2818 - accuracy: 0.2640 - val_loss: 5.5077 - val_accuracy: 0.0856\n",
            "Epoch 80/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2675 - accuracy: 0.2647 - val_loss: 5.5234 - val_accuracy: 0.0859\n",
            "Epoch 81/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2542 - accuracy: 0.2675 - val_loss: 5.5329 - val_accuracy: 0.0854\n",
            "Epoch 82/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2403 - accuracy: 0.2693 - val_loss: 5.5408 - val_accuracy: 0.0865\n",
            "Epoch 83/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2266 - accuracy: 0.2708 - val_loss: 5.5527 - val_accuracy: 0.0871\n",
            "Epoch 84/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2132 - accuracy: 0.2721 - val_loss: 5.5641 - val_accuracy: 0.0859\n",
            "Epoch 85/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2010 - accuracy: 0.2733 - val_loss: 5.5690 - val_accuracy: 0.0871\n",
            "Epoch 86/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1893 - accuracy: 0.2763 - val_loss: 5.5837 - val_accuracy: 0.0871\n",
            "Epoch 87/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1767 - accuracy: 0.2780 - val_loss: 5.5948 - val_accuracy: 0.0874\n",
            "Epoch 88/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1654 - accuracy: 0.2785 - val_loss: 5.6064 - val_accuracy: 0.0874\n",
            "Epoch 89/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1513 - accuracy: 0.2802 - val_loss: 5.6106 - val_accuracy: 0.0871\n",
            "Epoch 90/200\n",
            "1148/1148 [==============================] - 3s 3ms/step - loss: 2.1416 - accuracy: 0.2819 - val_loss: 5.6097 - val_accuracy: 0.0859\n",
            "Epoch 91/200\n",
            "1148/1148 [==============================] - 3s 3ms/step - loss: 2.1344 - accuracy: 0.2819 - val_loss: 5.6162 - val_accuracy: 0.0865\n",
            "Epoch 92/200\n",
            "1148/1148 [==============================] - 3s 3ms/step - loss: 2.1325 - accuracy: 0.2804 - val_loss: 5.6289 - val_accuracy: 0.0830\n",
            "Epoch 93/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1208 - accuracy: 0.2830 - val_loss: 5.6212 - val_accuracy: 0.0839\n",
            "Epoch 94/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1050 - accuracy: 0.2829 - val_loss: 5.6420 - val_accuracy: 0.0842\n",
            "Epoch 95/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0918 - accuracy: 0.2880 - val_loss: 5.6569 - val_accuracy: 0.0865\n",
            "Epoch 96/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0792 - accuracy: 0.2890 - val_loss: 5.6654 - val_accuracy: 0.0842\n",
            "Epoch 97/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0645 - accuracy: 0.2912 - val_loss: 5.6753 - val_accuracy: 0.0854\n",
            "Epoch 98/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0527 - accuracy: 0.2921 - val_loss: 5.6909 - val_accuracy: 0.0833\n",
            "Epoch 99/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0407 - accuracy: 0.2933 - val_loss: 5.6954 - val_accuracy: 0.0830\n",
            "Epoch 100/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0306 - accuracy: 0.2954 - val_loss: 5.6933 - val_accuracy: 0.0830\n",
            "Epoch 101/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0198 - accuracy: 0.2970 - val_loss: 5.7030 - val_accuracy: 0.0839\n",
            "Epoch 102/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0091 - accuracy: 0.2996 - val_loss: 5.7182 - val_accuracy: 0.0839\n",
            "Epoch 103/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9982 - accuracy: 0.3000 - val_loss: 5.7231 - val_accuracy: 0.0842\n",
            "Epoch 104/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9882 - accuracy: 0.3006 - val_loss: 5.7202 - val_accuracy: 0.0839\n",
            "Epoch 105/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9778 - accuracy: 0.3022 - val_loss: 5.7327 - val_accuracy: 0.0830\n",
            "Epoch 106/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9682 - accuracy: 0.3044 - val_loss: 5.7452 - val_accuracy: 0.0845\n",
            "Epoch 107/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9603 - accuracy: 0.3055 - val_loss: 5.7564 - val_accuracy: 0.0842\n",
            "Epoch 108/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9530 - accuracy: 0.3066 - val_loss: 5.7751 - val_accuracy: 0.0859\n",
            "Epoch 109/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9437 - accuracy: 0.3067 - val_loss: 5.7710 - val_accuracy: 0.0848\n",
            "Epoch 110/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9328 - accuracy: 0.3094 - val_loss: 5.7796 - val_accuracy: 0.0856\n",
            "Epoch 111/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9232 - accuracy: 0.3099 - val_loss: 5.7894 - val_accuracy: 0.0836\n",
            "Epoch 112/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9124 - accuracy: 0.3113 - val_loss: 5.8030 - val_accuracy: 0.0868\n",
            "Epoch 113/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9040 - accuracy: 0.3150 - val_loss: 5.8093 - val_accuracy: 0.0830\n",
            "Epoch 114/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8942 - accuracy: 0.3143 - val_loss: 5.8159 - val_accuracy: 0.0851\n",
            "Epoch 115/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8862 - accuracy: 0.3156 - val_loss: 5.8255 - val_accuracy: 0.0862\n",
            "Epoch 116/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8788 - accuracy: 0.3169 - val_loss: 5.8274 - val_accuracy: 0.0862\n",
            "Epoch 117/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8697 - accuracy: 0.3186 - val_loss: 5.8384 - val_accuracy: 0.0854\n",
            "Epoch 118/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8620 - accuracy: 0.3196 - val_loss: 5.8498 - val_accuracy: 0.0865\n",
            "Epoch 119/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8547 - accuracy: 0.3217 - val_loss: 5.8496 - val_accuracy: 0.0859\n",
            "Epoch 120/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8491 - accuracy: 0.3213 - val_loss: 5.8645 - val_accuracy: 0.0854\n",
            "Epoch 121/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8398 - accuracy: 0.3224 - val_loss: 5.8819 - val_accuracy: 0.0830\n",
            "Epoch 122/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8319 - accuracy: 0.3230 - val_loss: 5.8881 - val_accuracy: 0.0833\n",
            "Epoch 123/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8248 - accuracy: 0.3253 - val_loss: 5.8896 - val_accuracy: 0.0833\n",
            "Epoch 124/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8156 - accuracy: 0.3272 - val_loss: 5.8886 - val_accuracy: 0.0839\n",
            "Epoch 125/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8059 - accuracy: 0.3300 - val_loss: 5.9056 - val_accuracy: 0.0830\n",
            "Epoch 126/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7979 - accuracy: 0.3300 - val_loss: 5.9161 - val_accuracy: 0.0836\n",
            "Epoch 127/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7906 - accuracy: 0.3328 - val_loss: 5.9196 - val_accuracy: 0.0825\n",
            "Epoch 128/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7841 - accuracy: 0.3326 - val_loss: 5.9235 - val_accuracy: 0.0825\n",
            "Epoch 129/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7747 - accuracy: 0.3338 - val_loss: 5.9356 - val_accuracy: 0.0830\n",
            "Epoch 130/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7675 - accuracy: 0.3359 - val_loss: 5.9423 - val_accuracy: 0.0830\n",
            "Epoch 131/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7598 - accuracy: 0.3371 - val_loss: 5.9550 - val_accuracy: 0.0813\n",
            "Epoch 132/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7526 - accuracy: 0.3378 - val_loss: 5.9557 - val_accuracy: 0.0825\n",
            "Epoch 133/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7470 - accuracy: 0.3396 - val_loss: 5.9658 - val_accuracy: 0.0804\n",
            "Epoch 134/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7402 - accuracy: 0.3404 - val_loss: 5.9740 - val_accuracy: 0.0816\n",
            "Epoch 135/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7325 - accuracy: 0.3396 - val_loss: 5.9675 - val_accuracy: 0.0804\n",
            "Epoch 136/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7261 - accuracy: 0.3421 - val_loss: 5.9772 - val_accuracy: 0.0828\n",
            "Epoch 137/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7208 - accuracy: 0.3441 - val_loss: 5.9835 - val_accuracy: 0.0833\n",
            "Epoch 138/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7134 - accuracy: 0.3431 - val_loss: 5.9920 - val_accuracy: 0.0822\n",
            "Epoch 139/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7072 - accuracy: 0.3443 - val_loss: 6.0014 - val_accuracy: 0.0816\n",
            "Epoch 140/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7011 - accuracy: 0.3460 - val_loss: 6.0017 - val_accuracy: 0.0836\n",
            "Epoch 141/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6933 - accuracy: 0.3455 - val_loss: 6.0142 - val_accuracy: 0.0825\n",
            "Epoch 142/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6873 - accuracy: 0.3479 - val_loss: 6.0112 - val_accuracy: 0.0796\n",
            "Epoch 143/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6815 - accuracy: 0.3485 - val_loss: 6.0271 - val_accuracy: 0.0793\n",
            "Epoch 144/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6798 - accuracy: 0.3471 - val_loss: 6.0437 - val_accuracy: 0.0807\n",
            "Epoch 145/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6735 - accuracy: 0.3471 - val_loss: 6.0435 - val_accuracy: 0.0828\n",
            "Epoch 146/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6637 - accuracy: 0.3516 - val_loss: 6.0454 - val_accuracy: 0.0822\n",
            "Epoch 147/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6584 - accuracy: 0.3505 - val_loss: 6.0517 - val_accuracy: 0.0825\n",
            "Epoch 148/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6529 - accuracy: 0.3539 - val_loss: 6.0606 - val_accuracy: 0.0799\n",
            "Epoch 149/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6481 - accuracy: 0.3551 - val_loss: 6.0639 - val_accuracy: 0.0833\n",
            "Epoch 150/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6421 - accuracy: 0.3557 - val_loss: 6.0727 - val_accuracy: 0.0802\n",
            "Epoch 151/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6345 - accuracy: 0.3570 - val_loss: 6.0900 - val_accuracy: 0.0810\n",
            "Epoch 152/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6290 - accuracy: 0.3565 - val_loss: 6.0965 - val_accuracy: 0.0816\n",
            "Epoch 153/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6217 - accuracy: 0.3579 - val_loss: 6.0968 - val_accuracy: 0.0825\n",
            "Epoch 154/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6172 - accuracy: 0.3599 - val_loss: 6.1073 - val_accuracy: 0.0816\n",
            "Epoch 155/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6121 - accuracy: 0.3585 - val_loss: 6.1134 - val_accuracy: 0.0825\n",
            "Epoch 156/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6084 - accuracy: 0.3598 - val_loss: 6.1222 - val_accuracy: 0.0822\n",
            "Epoch 157/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6019 - accuracy: 0.3606 - val_loss: 6.1254 - val_accuracy: 0.0822\n",
            "Epoch 158/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5957 - accuracy: 0.3619 - val_loss: 6.1287 - val_accuracy: 0.0790\n",
            "Epoch 159/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5926 - accuracy: 0.3634 - val_loss: 6.1378 - val_accuracy: 0.0787\n",
            "Epoch 160/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5852 - accuracy: 0.3653 - val_loss: 6.1460 - val_accuracy: 0.0784\n",
            "Epoch 161/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5795 - accuracy: 0.3664 - val_loss: 6.1472 - val_accuracy: 0.0796\n",
            "Epoch 162/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5754 - accuracy: 0.3661 - val_loss: 6.1652 - val_accuracy: 0.0787\n",
            "Epoch 163/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5709 - accuracy: 0.3654 - val_loss: 6.1621 - val_accuracy: 0.0790\n",
            "Epoch 164/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5662 - accuracy: 0.3679 - val_loss: 6.1884 - val_accuracy: 0.0810\n",
            "Epoch 165/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5622 - accuracy: 0.3677 - val_loss: 6.1756 - val_accuracy: 0.0790\n",
            "Epoch 166/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5564 - accuracy: 0.3702 - val_loss: 6.1944 - val_accuracy: 0.0802\n",
            "Epoch 167/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5489 - accuracy: 0.3723 - val_loss: 6.1951 - val_accuracy: 0.0813\n",
            "Epoch 168/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5465 - accuracy: 0.3710 - val_loss: 6.1953 - val_accuracy: 0.0793\n",
            "Epoch 169/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5451 - accuracy: 0.3719 - val_loss: 6.2070 - val_accuracy: 0.0804\n",
            "Epoch 170/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5398 - accuracy: 0.3719 - val_loss: 6.2164 - val_accuracy: 0.0810\n",
            "Epoch 171/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5383 - accuracy: 0.3715 - val_loss: 6.2194 - val_accuracy: 0.0784\n",
            "Epoch 172/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5317 - accuracy: 0.3724 - val_loss: 6.2268 - val_accuracy: 0.0793\n",
            "Epoch 173/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5273 - accuracy: 0.3758 - val_loss: 6.2399 - val_accuracy: 0.0778\n",
            "Epoch 174/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5229 - accuracy: 0.3736 - val_loss: 6.2316 - val_accuracy: 0.0799\n",
            "Epoch 175/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5175 - accuracy: 0.3779 - val_loss: 6.2513 - val_accuracy: 0.0796\n",
            "Epoch 176/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5115 - accuracy: 0.3797 - val_loss: 6.2504 - val_accuracy: 0.0796\n",
            "Epoch 177/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5046 - accuracy: 0.3789 - val_loss: 6.2623 - val_accuracy: 0.0799\n",
            "Epoch 178/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4987 - accuracy: 0.3818 - val_loss: 6.2703 - val_accuracy: 0.0807\n",
            "Epoch 179/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4943 - accuracy: 0.3809 - val_loss: 6.2859 - val_accuracy: 0.0790\n",
            "Epoch 180/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4911 - accuracy: 0.3831 - val_loss: 6.2754 - val_accuracy: 0.0802\n",
            "Epoch 181/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4892 - accuracy: 0.3815 - val_loss: 6.2978 - val_accuracy: 0.0799\n",
            "Epoch 182/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4824 - accuracy: 0.3825 - val_loss: 6.2942 - val_accuracy: 0.0790\n",
            "Epoch 183/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4766 - accuracy: 0.3860 - val_loss: 6.3046 - val_accuracy: 0.0807\n",
            "Epoch 184/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4750 - accuracy: 0.3844 - val_loss: 6.3034 - val_accuracy: 0.0802\n",
            "Epoch 185/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4719 - accuracy: 0.3857 - val_loss: 6.3103 - val_accuracy: 0.0784\n",
            "Epoch 186/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4657 - accuracy: 0.3856 - val_loss: 6.3199 - val_accuracy: 0.0781\n",
            "Epoch 187/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4604 - accuracy: 0.3878 - val_loss: 6.3185 - val_accuracy: 0.0796\n",
            "Epoch 188/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4573 - accuracy: 0.3886 - val_loss: 6.3302 - val_accuracy: 0.0784\n",
            "Epoch 189/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4491 - accuracy: 0.3903 - val_loss: 6.3295 - val_accuracy: 0.0807\n",
            "Epoch 190/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4474 - accuracy: 0.3909 - val_loss: 6.3389 - val_accuracy: 0.0784\n",
            "Epoch 191/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4436 - accuracy: 0.3902 - val_loss: 6.3376 - val_accuracy: 0.0819\n",
            "Epoch 192/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4396 - accuracy: 0.3923 - val_loss: 6.3454 - val_accuracy: 0.0790\n",
            "Epoch 193/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4346 - accuracy: 0.3912 - val_loss: 6.3542 - val_accuracy: 0.0784\n",
            "Epoch 194/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4306 - accuracy: 0.3934 - val_loss: 6.3567 - val_accuracy: 0.0778\n",
            "Epoch 195/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4246 - accuracy: 0.3937 - val_loss: 6.3640 - val_accuracy: 0.0804\n",
            "Epoch 196/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4227 - accuracy: 0.3953 - val_loss: 6.3667 - val_accuracy: 0.0784\n",
            "Epoch 197/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4189 - accuracy: 0.3954 - val_loss: 6.3800 - val_accuracy: 0.0784\n",
            "Epoch 198/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4180 - accuracy: 0.3976 - val_loss: 6.3711 - val_accuracy: 0.0802\n",
            "Epoch 199/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4164 - accuracy: 0.3961 - val_loss: 6.3786 - val_accuracy: 0.0784\n",
            "Epoch 200/200\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.4080 - accuracy: 0.4009 - val_loss: 6.3997 - val_accuracy: 0.0787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdjsrsLeJzdL",
        "colab_type": "text"
      },
      "source": [
        "Plotting the accuracy and loss curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvoROL3RL6e8",
        "colab_type": "text"
      },
      "source": [
        "As it is poetry so predicting the correct next word will always be hard here so accuracy will be impacted by this much"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I0ycbzVJww-",
        "colab_type": "code",
        "outputId": "1884296c-443e-49b8-b365-6ab78989c378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(ht.history[\"loss\"],label = \"loss\")\n",
        "plt.plot(ht.history[\"val_loss\"],label = \"val_loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG3nVqDBKa4R",
        "colab_type": "code",
        "outputId": "54c93401-5ebf-434d-bc4f-d149473a84b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(ht.history[\"accuracy\"],label = \"accuracy\")\n",
        "plt.plot(ht.history[\"val_accuracy\"],label = \"validation_accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq6BP4f7Kot1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"model_gen.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxBQWLbuh8DX",
        "colab_type": "text"
      },
      "source": [
        "Here what is happening is we are feding a input sentence and we are getting back the prediction for the next word all at same time and basically what we need is to make a model where we will feed a word at a time and will get prediction for it and then will feed the predicted word to get the new prediction of the next word and so on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y83xM_glh_QX",
        "colab_type": "text"
      },
      "source": [
        "Now we will build a model to which we will give a shape of 1 as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "146DvvWuh-dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = Input(shape = (1,))\n",
        "x = embedding_layer(input1)\n",
        "x,h,c = lstm(x,[initial_h,initial_c])\n",
        "output1 = dense(x)\n",
        "model_x = Model([input1,initial_h,initial_c],[output1,h,c])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCJAhMtLk6OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reversing the word2idx dictionary\n",
        "idx2word = {v:k for k,v in word2idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA30kSMDlNZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#THis sample line function gonna generate one line of output\n",
        "def sample_line():\n",
        "  #Defining the initial inputs\n",
        "  #Initial word from starting will be the start of sentence token and initial cell state and initail hidden state will be vector of zeros\n",
        "  np_input = np.array([[word2idx[\"<sos>\"]]])\n",
        "  h = np.zeros((1,M))\n",
        "  c = np.zeros((1,M))\n",
        "  eos = word2idx[\"<eos>\"]\n",
        "  #At each step after predicting the first word what we gonna do is input the current cell state and the hidden state from previous stamp\n",
        "  #We will be storing the generate output sentences in outputs vector\n",
        "  outputs = []\n",
        "  for _ in range(max_sequence_length):\n",
        "    #o is list of probablities for the next word\n",
        "    o, h, c = model_x.predict([np_input,h,c])\n",
        "    #It is very much possible to get a very high probablity for index 0 which generally doesn't correspond to any word\n",
        "    probs = o[0,0]\n",
        "    if np.argmax(probs) == 0:\n",
        "      print(\"##\")\n",
        "    probs[0] = 0\n",
        "    probs /= probs.sum()\n",
        "    #Sampling the next word randomly\n",
        "    idx = np.random.choice(len(probs),p=probs)\n",
        "    if idx == eos:\n",
        "      break\n",
        "    #Output\n",
        "    outputs.append(idx2word.get(idx,\"<## %s>\"%idx))\n",
        "    #Next input the model will be the word predicted\n",
        "    np_input[0,0] = idx\n",
        "  return \" \".join(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy0uMu7qoIhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "c5de2161-2c2a-4783-b7b2-08a6ed65ed3c"
      },
      "source": [
        "while True:\n",
        "  #As we 4 line perverse in poetry so we will be generating 4 line at a time\n",
        "  for _ in range(4):\n",
        "    print(sample_line())\n",
        "  \n",
        "  ans = input(\"ppf generate another \")\n",
        "  if ans and ans[0].lower().startswith(\"n\"):\n",
        "    break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "now lupine ears settlement?\n",
            "i see what they creaking i fade,\n",
            "but smith, or someone, called them straight away\n",
            "anyone else, but out of every emptied double trouble's\n",
            "ppf generate another y\n",
            "though not a grave down in trees and wet,\n",
            "two standing to mind\n",
            "to like the night all fresh\n",
            "the only get they say had gone before i've brought you to\n",
            "ppf generate another y\n",
            "ppf generate another \n",
            "moisture and still be undone,\n",
            "from that would have shifted has familiar spirits\n",
            "if i mistrusted he found on the whole thing i stand together\n",
            "'i only bewitched so mormon swimming or three?'\n",
            "'i'm aftermath,\n",
            "by picking the way of the cellar forty years ago as that?'\n",
            "in garden upstairs in meeting of oil of make-believe,\n",
            "what i see what they sometimes because to shelter alone he snapped\n",
            "ppf generate another y\n",
            "and scuttled just as well withdrawing laces\n",
            "moisture and hold the ideals,\n",
            "'i can help it.\n",
            "good-looking books like that.' he was petered out,\n",
            "ppf generate another n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA74aOQayEzp",
        "colab_type": "text"
      },
      "source": [
        "The issue of accuracy is because the task is poetry generation and predicting next word in poetry is pretty difficult even for humans reading some previous poetry so same happens with algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T2r75q3vr-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}